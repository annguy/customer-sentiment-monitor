[parameters]
; Feature_type extracted from feature_matrix (both_feature_types, enterprise or log)
feature_type = both_feature_types
; Number of weeks used for creating validation
num_of_val_weeks = 10
; The size of train and validation set. If fixed_window_in_weeks equal to -1 then use all available samples
fixed_window_in_weeks = 52
; The number of weeks used for creating labels
lookahead_window_len = 2
; The model (XGBoost or RandomForestClassifier or LSTM)
model_name = LSTM
; The late fusion flag
late_fusion_flag = true
; The number of trials for optuna TPESampler
number_of_trials = 1
; The first week in weekly analysis YYYY-WW (according to ISO8601) 1997-14
first_week = 1997-14
; The last week in weekly analysis YYYY-WW (according to ISO8601) 1998-14
last_week = 1997-14
; In case of the terminated experiments, there is the possibility to continue training. Cont_week is equal to None or
; pred_time (ISO CW) if pred_time-1 has been already finished
cont_week = None
; number of epochs used for LSTM early stopping
patience = 15
; MedianPruner parameters
n_startup_trials=10
n_warmup_steps=15
interval_steps=5
; The parameters for hyperparameter tuning by optuna in case of RandomForestClassifier
params_RandomForestClassifier = {'max_samples': ['uniform', 0.1, 1.0],
                                 'max_depth': ['int', 1, 50],
                                 'n_estimators': ['int', 100, 1000],
                                 'random_state': ['categorical', [0]],
                                 'verbose': ['categorical', [0]],
                                 'criterion': ['categorical', ['gini', 'entropy']],
                                 'n_jobs': ['categorical', [30]],
                                 'ratio': ['uniform', 0.1, 1.0],
                                 'sampling_strategy': ['categorical', ['over', 'over_SMOTE', 'under']]}
; The parameters for hyperparameter tuning by optuna in case of XGBoost
params_XGBoost = {'max_depth': ['int', 1, 50],
                  'learning_rate': ['loguniform', 1e-3, 1],
                  'n_estimators': ['int', 100, 1000],
                  'colsample_bytree': ['uniform', 0.5, 1.0],
                  'subsample': ['uniform', 0.5, 1.0],
                  'reg_lambda': ['loguniform', 0.1, 10.0],
                  'random_state': ['categorical', [0]],
                  'objective': ['categorical', ['binary:logistic']],
                  'n_jobs': ['categorical', [30]],
                  'ratio': ['uniform', 0.1, 1.0],
                  'sampling_strategy': ['categorical', ['over', 'over_SMOTE', 'under']]}

; The parameters for hyperparameter tuning by optuna in case of LSTM
params_LSTM = {'batch_size': ['categorical', [32, 64, 128]],
               'num_epochs': ['fixed', 150],
              'early_stopping_epochs': ['fixed', 25],
              'early_stopping': ['fixed', True],
              'hidden_dim': ['int', 16, 128],
              'learning_rate': ['loguniform', 1e-5, 1],
              'weight_decay': ['uniform', 0, 0.75],
              'dropout_prob': ['uniform', 0, 0.75],
               'lstm_layer': ['int', 1, 2],
               'lstm_bidirectional': ['categorical', [True, False]],
              'ratio': ['uniform', 0.1, 1.0],
              'sampling_strategy': ['categorical', ['over', 'under']]}
